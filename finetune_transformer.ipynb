{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d277e48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_str = \"ufal/robeczech-base\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cdb0c1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ufal/robeczech-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ufal/robeczech-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_str,\n",
    "    num_labels=7,\n",
    "    output_hidden_states=True\n",
    ")\n",
    "model = AutoModelForSequenceClassification.\\\n",
    "    from_pretrained(model_str, config=config).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed8b844c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-300a50ec4e320bb6\n",
      "Reusing dataset pandas (/home/jacob/.cache/huggingface/datasets/pandas/default-300a50ec4e320bb6/0.0.0/6197c1e855b639d75a767140856841a562b7a71d129104973fe1962594877ade)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101654b846ed4887902b2e060e1b72c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a199bf798aa942158ebb39ad6db0a417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"pandas\", data_files=\"words_df.pkl\")\n",
    "\n",
    "def tokenize_datapoint(datapoint):\n",
    "    return tokenizer(datapoint[\"word\"], padding=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_datapoint, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(\"word\")\n",
    "\n",
    "tokenized_dataset = tokenized_dataset[\"train\"].train_test_split(train_size=0.8)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"cases\", \"labels\")\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d34fb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\"czech-finetune\", num_train_epochs=6)\n",
    "\n",
    "from transformers import Trainer, PreTrainedModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def unwrap_model(model):\n",
    "    \"\"\"\n",
    "    Recursively unwraps a model from potential containers (as used in distributed training).\n",
    "\n",
    "    Args:\n",
    "        model (`torch.nn.Module`): The model to unwrap.\n",
    "    \"\"\"\n",
    "    # since there could be multiple levels of wrapping, unwrap recursively\n",
    "    if hasattr(model, \"module\"):\n",
    "        return unwrap_model(model.module)\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "# custom trainer to avoid batch size error\n",
    "# shoutouts to https://discuss.huggingface.co/t/1653/2\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "        )\n",
    "        labels = inputs['labels'].float()\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(outputs['logits'], labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    def _save(self, output_dir=None, state_dict=None):\n",
    "        # If we are executing this function, we are the process zero, so we don't check for that.\n",
    "        output_dir = output_dir if output_dir is not None else self.args.output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Saving model checkpoint to {output_dir}\")\n",
    "        # Save a trained model and configuration using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        self.model.save_pretrained(output_dir, state_dict=state_dict)\n",
    "        #if self.tokenizer is not None:\n",
    "        #`   self.tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "\n",
    "trainer = CustomTrainer(model, training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553a408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from ./czech-finetune/checkpoint-2000).\n",
      "***** Running training *****\n",
      "  Num examples = 5509\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4134\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 2\n",
      "  Continuing training from global step 2000\n",
      "  Will skip the first 2 epochs then the first 622 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9b4da78c804506bee945cf77ef5151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/622 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3027' max='4134' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3027/4134 02:10 < 02:21, 7.83 it/s, Epoch 4.39/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.074700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.064100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in czech-finetune/checkpoint-2500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to czech-finetune/checkpoint-2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in czech-finetune/checkpoint-2500/pytorch_model.bin\n",
      "Configuration saved in czech-finetune/checkpoint-3000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to czech-finetune/checkpoint-3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in czech-finetune/checkpoint-3000/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.train(\"./czech-finetune/checkpoint-2000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66d606fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5.9616e-01, 1.1494e-04, 5.1304e-05, 4.0341e-01, 1.3781e-04, 7.5256e-05,\n",
       "         4.7899e-05]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(**tokenizer(\"den\", return_tensors=\"pt\").to(\"cuda:0\"))[\"logits\"]\n",
    "import torch\n",
    "torch.nn.functional.softmax(logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
